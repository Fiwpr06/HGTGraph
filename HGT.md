# T√†i li·ªáu Chi ti·∫øt M√£ ngu·ªìn HGT (Heterogeneous Graph Transformer)

T√†i li·ªáu n√†y cung c·∫•p ph√¢n t√≠ch chi ti·∫øt v·ªÅ c√°c t·ªáp tin ngu·ªìn li√™n quan ƒë·∫øn m√¥ h√¨nh HGT trong d·ª± √°n. T√†i li·ªáu ƒë∆∞·ª£c bi√™n so·∫°n nh·∫±m m·ª•c ƒë√≠ch gi√°o d·ª•c, gi·∫£i th√≠ch r√µ r√†ng t·ª´ng ph∆∞∆°ng th·ª©c, lu·ªìng x·ª≠ l√Ω v√† vai tr√≤ c·ªßa ch√∫ng trong h·ªá th·ªëng.

---

## 1. hgt_evaluation.py

T·ªáp n√†y ch·ªãu tr√°ch nhi·ªám ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh v√† tr·ª±c quan h√≥a k·∫øt qu·∫£. N√≥ s·ª≠ d·ª•ng c√°c th∆∞ vi·ªán nh∆∞ `matplotlib`, `seaborn` v√† `sklearn` ƒë·ªÉ t·∫°o bi·ªÉu ƒë·ªì v√† t√≠nh to√°n c√°c ch·ªâ s·ªë th·ªëng k√™.

### 1.1 `HGTEvaluator.__init__`

```python
    def __init__(self, model, graph, test_data, edge_type, device='cpu'):
        self.model = model.to(device)
        self.graph = graph
        self.test_data = test_data.to(device)
        self.edge_type = edge_type
        self.device = device
```

- **M·ª•c ƒë√≠ch:** Kh·ªüi t·∫°o ƒë·ªëi t∆∞·ª£ng `HGTEvaluator`, thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng ƒë√°nh gi√°.
- **Gi·∫£i th√≠ch chi ti·∫øt:**
  - ƒê√¢y l√† h√†m kh·ªüi t·∫°o (constructor) c·ªßa l·ªõp.
  - **Tham s·ªë:**
    - `model`: M√¥ h√¨nh HGT ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán.
    - `graph`: To√†n b·ªô c·∫•u tr√∫c ƒë·ªì th·ªã (HeteroData).
    - `test_data`: T·∫≠p d·ªØ li·ªáu ki·ªÉm th·ª≠ (ƒë√£ ƒë∆∞·ª£c t√°ch ra t·ª´ qu√° tr√¨nh training).
    - `edge_type`: Lo·∫°i c·∫°nh (quan h·ªá) m√† ch√∫ng ta mu·ªën ƒë√°nh gi√° d·ª± ƒëo√°n (v√≠ d·ª•: `('job', 'similar_to', 'job')`).
    - `device`: Thi·∫øt b·ªã ch·∫°y t√≠nh to√°n ('cpu' ho·∫∑c 'cuda').
  - **Logic:**
    - `self.model = model.to(device)`: Chuy·ªÉn m√¥ h√¨nh sang thi·∫øt b·ªã t√≠nh to√°n (GPU/CPU).
    - L∆∞u tr·ªØ c√°c tham s·ªë c√≤n l·∫°i v√†o thu·ªôc t√≠nh c·ªßa instance ƒë·ªÉ s·ª≠ d·ª•ng trong c√°c ph∆∞∆°ng th·ª©c kh√°c.

### 1.2 `HGTEvaluator.get_predictions`

```python
    @torch.no_grad()
    def get_predictions(self):
        """L·∫•y d·ª± ƒëo√°n c·ªßa m√¥ h√¨nh tr√™n t·∫≠p test"""
        self.model.eval()

        edge_label_index = self.test_data[self.edge_type].edge_label_index
        edge_label = self.test_data[self.edge_type].edge_label

        x_dict = {
            'job': self.test_data['job'].x,
            'company': self.test_data['company'].x,
            'location': self.test_data['location'].x,
        }

        edge_index_dict = {
            key: self.test_data[key].edge_index
            for key in self.test_data.edge_types
        }

        pred = self.model(x_dict, edge_index_dict, edge_label_index, self.edge_type)
        pred_probs = torch.sigmoid(pred).cpu().numpy()
        labels = edge_label.cpu().numpy()

        return pred_probs, labels
```

- **M·ª•c ƒë√≠ch:** Th·ª±c hi·ªán d·ª± ƒëo√°n tr√™n t·∫≠p d·ªØ li·ªáu ki·ªÉm th·ª≠ (test set) v√† tr·∫£ v·ªÅ x√°c su·∫•t d·ª± ƒëo√°n c√πng nh√£n th·ª±c t·∫ø.
- **Gi·∫£i th√≠ch chi ti·∫øt:**
  - `@torch.no_grad()`: Decorator b√°o cho PyTorch kh√¥ng c·∫ßn t√≠nh to√°n gradient, gi√∫p ti·∫øt ki·ªám b·ªô nh·ªõ v√† tƒÉng t·ªëc ƒë·ªô v√¨ ƒë√¢y l√† b∆∞·ªõc ƒë√°nh gi√° (inference), kh√¥ng ph·∫£i hu·∫•n luy·ªán.
  - `self.model.eval()`: Chuy·ªÉn m√¥ h√¨nh sang ch·∫ø ƒë·ªô ƒë√°nh gi√° (t·∫Øt Dropout, Batch Norm ho·∫°t ƒë·ªông theo th·ªëng k√™ to√†n c·ª•c).
  - **Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o:**
    - `edge_label_index`: Ch·ªâ s·ªë c√°c c·∫°nh c·∫ßn d·ª± ƒëo√°n trong t·∫≠p test.
    - `edge_label`: Nh√£n th·ª±c t·∫ø (1 l√† c√≥ li√™n k·∫øt, 0 l√† kh√¥ng c√≥) c·ªßa c√°c c·∫°nh ƒë√≥.
    - `x_dict`: Dictionary ch·ª©a ƒë·∫∑c tr∆∞ng (features) c·ªßa t·ª´ng lo·∫°i node (job, company, location).
    - `edge_index_dict`: Dictionary ch·ª©a c·∫•u tr√∫c k·∫øt n·ªëi c·ªßa ƒë·ªì th·ªã trong t·∫≠p test.
  - **Th·ª±c thi m√¥ h√¨nh:**
    - `pred = self.model(...)`: G·ªçi h√†m `forward` c·ªßa m√¥ h√¨nh ƒë·ªÉ l·∫•y k·∫øt qu·∫£ (logits).
    - `torch.sigmoid(pred)`: √Åp d·ª•ng h√†m Sigmoid ƒë·ªÉ chuy·ªÉn ƒë·ªïi logits th√†nh x√°c su·∫•t (0.0 ƒë·∫øn 1.0).
    - `.cpu().numpy()`: Chuy·ªÉn Tensor t·ª´ GPU v·ªÅ CPU v√† ƒë·ªïi sang ƒë·ªãnh d·∫°ng NumPy array ƒë·ªÉ d·ªÖ x·ª≠ l√Ω v·ªõi th∆∞ vi·ªán sklearn.
  - **Gi√° tr·ªã tr·∫£ v·ªÅ:**
    - `pred_probs`: M·∫£ng x√°c su·∫•t d·ª± ƒëo√°n.
    - `labels`: M·∫£ng nh√£n th·ª±c t·∫ø.

### 1.3 `HGTEvaluator.get_embeddings`

```python
    @torch.no_grad()
    def get_embeddings(self):
        """L·∫•y embeddings c·ªßa c√°c node t·ª´ HGT encoder"""
        self.model.eval()

        x_dict = {
            'job': self.graph['job'].x.to(self.device),
            'company': self.graph['company'].x.to(self.device),
            'location': self.graph['location'].x.to(self.device),
        }

        edge_index_dict = {
            ('job', 'posted_by', 'company'): self.graph['job', 'posted_by', 'company'].edge_index.to(self.device),
            ('company', 'posts', 'job'): self.graph['company', 'posts', 'job'].edge_index.to(self.device),
            ('job', 'located_in', 'location'): self.graph['job', 'located_in', 'location'].edge_index.to(self.device),
            ('location', 'has', 'job'): self.graph['location', 'has', 'job'].edge_index.to(self.device),
            ('job', 'similar_to', 'job'): self.graph['job', 'similar_to', 'job'].edge_index.to(self.device),
        }

        embeddings = self.model.encode(x_dict, edge_index_dict)

        # Chuy·ªÉn sang numpy
        embeddings_np = {
            key: emb.cpu().numpy()
            for key, emb in embeddings.items()
        }

        return embeddings_np
```

- **M·ª•c ƒë√≠ch:** Tr√≠ch xu·∫•t vector ƒë·∫∑c tr∆∞ng (embeddings) c·ªßa t·∫•t c·∫£ c√°c node sau khi ƒë√£ ƒëi qua m√¥ h√¨nh HGT. Embeddings n√†y ch·ª©a th√¥ng tin ng·ªØ nghƒ©a t·ªïng h·ª£p t·ª´ ƒë·ªì th·ªã.
- **Gi·∫£i th√≠ch chi ti·∫øt:**
  - Kh√°c v·ªõi `get_predictions` ch·ªâ ch·∫°y tr√™n t·∫≠p test, h√†m n√†y s·ª≠ d·ª•ng `self.graph` (to√†n b·ªô ƒë·ªì th·ªã) ƒë·ªÉ t·∫°o embeddings cho t·∫•t c·∫£ c√°c n√∫t.
  - **Chu·∫©n b·ªã d·ªØ li·ªáu:**
    - T·∫°o `x_dict` v√† `edge_index_dict` t·ª´ to√†n b·ªô ƒë·ªì th·ªã g·ªëc, ƒë·∫£m b·∫£o chuy·ªÉn d·ªØ li·ªáu sang ƒë√∫ng `device`.
  - **Tr√≠ch xu·∫•t:**
    - `self.model.encode(...)`: G·ªçi ph∆∞∆°ng th·ª©c `encode` ri√™ng c·ªßa m√¥ h√¨nh HGT (ch·ªâ ch·∫°y ph·∫ßn Encoder, kh√¥ng ch·∫°y ph·∫ßn Predictor head).
  - **X·ª≠ l√Ω k·∫øt qu·∫£:**
    - V√≤ng l·∫∑p `for` chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ t·ª´ Tensor sang NumPy array cho t·ª´ng lo·∫°i node.
  - **Gi√° tr·ªã tr·∫£ v·ªÅ:** `embeddings_np` l√† m·ªôt dictionary ch·ª©a c√°c ma tr·∫≠n embedding cho 'job', 'company', 'location'.

### 1.4 `HGTEvaluator.plot_roc_pr_curves`

```python
    def plot_roc_pr_curves(self, save_path=None):
        """V·∫Ω ƒë∆∞·ªùng cong ROC v√† Precision-Recall"""
        pred_probs, labels = self.get_predictions()

        fig, axes = plt.subplots(1, 2, figsize=(14, 6))

        # ƒê∆∞·ªùng cong ROC
        fpr, tpr, _ = roc_curve(labels, pred_probs)
        from sklearn.metrics import roc_auc_score
        auc = roc_auc_score(labels, pred_probs)

        axes[0].plot(fpr, tpr, 'b-', linewidth=2, label=f'HGT (AUC = {auc:.4f})')
        axes[0].plot([0, 1], [0, 1], 'r--', linewidth=2, label='Ng·∫´u nhi√™n')
        axes[0].set_xlabel('T·ªâ l·ªá D∆∞∆°ng Gi·∫£ (FPR)', fontsize=12, fontweight='bold')
        axes[0].set_ylabel('T·ªâ l·ªá D∆∞∆°ng Th·ª±c (TPR)', fontsize=12, fontweight='bold')
        axes[0].set_title('ƒê∆∞·ªùng cong ROC - D·ª± ƒëo√°n Li√™n k·∫øt', fontsize=14, fontweight='bold')
        axes[0].legend(loc='lower right', fontsize=11)
        axes[0].grid(True, alpha=0.3)

        # ƒê∆∞·ªùng cong Precision-Recall
        precision, recall, _ = precision_recall_curve(labels, pred_probs)
        from sklearn.metrics import average_precision_score
        ap = average_precision_score(labels, pred_probs)

        axes[1].plot(recall, precision, 'b-', linewidth=2, label=f'HGT (AP = {ap:.4f})')
        axes[1].axhline(y=labels.mean(), color='r', linestyle='--', linewidth=2, label='Ng·∫´u nhi√™n')
        axes[1].set_xlabel('ƒê·ªô ph·ªß (Recall)', fontsize=12, fontweight='bold')
        axes[1].set_ylabel('ƒê·ªô ch√≠nh x√°c (Precision)', fontsize=12, fontweight='bold')
        axes[1].set_title('ƒê∆∞·ªùng cong Precision-Recall', fontsize=14, fontweight='bold')
        axes[1].legend(loc='lower left', fontsize=11)
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"‚úÖ ƒê∆∞·ªùng cong ROC & PR ƒë√£ l∆∞u t·∫°i {save_path}")

        plt.close()
```

- **M·ª•c ƒë√≠ch:** V·∫Ω hai bi·ªÉu ƒë·ªì quan tr·ªçng ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh ph√¢n lo·∫°i nh·ªã ph√¢n: ROC Curve v√† Precision-Recall Curve.
- **Gi·∫£i th√≠ch chi ti·∫øt:**
  - **IO/GUI:** S·ª≠ d·ª•ng `matplotlib` ƒë·ªÉ v·∫Ω h√¨nh.
  - **Logic:**
    1.  G·ªçi `get_predictions()` ƒë·ªÉ l·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n.
    2.  **Bi·ªÉu ƒë·ªì ROC (Receiver Operating Characteristic):**
        - T√≠nh `roc_curve` (False Positive Rate vs True Positive Rate) v√† `auc` (Area Under Curve). AUC c√†ng g·∫ßn 1.0 c√†ng t·ªët.
        - V·∫Ω ƒë∆∞·ªùng c∆° s·ªü (Random) m√†u ƒë·ªè ƒë·ª©t n√©t (ƒë·∫°i di·ªán cho vi·ªác ƒëo√°n m√≤).
    3.  **Bi·ªÉu ƒë·ªì Precision-Recall:**
        - T√≠nh `precision`, `recall` v√† ƒëi·ªÉm `Average Precision (AP)`. Bi·ªÉu ƒë·ªì n√†y quan tr·ªçng khi d·ªØ li·ªáu m·∫•t c√¢n b·∫±ng.
  - **L∆∞u file:** N·∫øu c√≥ tham s·ªë `save_path`, bi·ªÉu ƒë·ªì s·∫Ω ƒë∆∞·ª£c l∆∞u th√†nh file ·∫£nh (PNG/JPG).

### 1.5 `HGTEvaluator.plot_confusion_matrix`

```python
    def plot_confusion_matrix(self, threshold=0.5, save_path=None):
        """V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n (confusion matrix)"""
        pred_probs, labels = self.get_predictions()
        pred_labels = (pred_probs >= threshold).astype(int)

        cm = confusion_matrix(labels, pred_labels)

        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['√Çm t√≠nh', 'D∆∞∆°ng t√≠nh'],
                    yticklabels=['√Çm t√≠nh', 'D∆∞∆°ng t√≠nh'],
                    cbar_kws={'label': 'S·ªë l∆∞·ª£ng'})
        plt.xlabel('Nh√£n d·ª± ƒëo√°n', fontsize=12, fontweight='bold')
        plt.ylabel('Nh√£n th·ª±c t·∫ø', fontsize=12, fontweight='bold')
        plt.title(f'Ma tr·∫≠n Nh·∫ßm l·∫´n (ng∆∞·ª°ng={threshold})', fontsize=14, fontweight='bold')

        # Th√™m c√°c ch·ªâ s·ªë
        tn, fp, fn, tp = cm.ravel()
        accuracy = (tp + tn) / (tp + tn + fp + fn)
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        metrics_text = f"ƒê·ªô ch√≠nh x√°c: {accuracy:.4f}\nPrecision: {precision:.4f}\nRecall: {recall:.4f}\nF1-Score: {f1:.4f}"
        plt.text(2.5, 0.5, metrics_text, fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"‚úÖ Ma tr·∫≠n nh·∫ßm l·∫´n ƒë√£ l∆∞u t·∫°i {save_path}")

        plt.close()
```

- **M·ª•c ƒë√≠ch:** Hi·ªÉn th·ªã Confusion Matrix ƒë·ªÉ xem chi ti·∫øt s·ªë l∆∞·ª£ng True Positive, False Positive, True Negative, v√† False Negative.
- **Gi·∫£i th√≠ch chi ti·∫øt:**
  - **Tham s·ªë:** `threshold`: Ng∆∞·ª°ng ƒë·ªÉ quy·∫øt ƒë·ªãnh ph√¢n l·ªõp (m·∫∑c ƒë·ªãnh 0.5). N·∫øu x√°c su·∫•t >= 0.5 th√¨ coi l√† Positive.
  - **Logic:**
    1.  Chuy·ªÉn x√°c su·∫•t th√†nh nh√£n 0/1 d·ª±a tr√™n ng∆∞·ª°ng.
    2.  S·ª≠ d·ª•ng `seaborn.heatmap` ƒë·ªÉ v·∫Ω ma tr·∫≠n m√†u s·∫Øc tr·ª±c quan.
    3.  T√≠nh to√°n th·ªß c√¥ng c√°c ch·ªâ s·ªë Accuracy, Precision, Recall, F1-Score t·ª´ c√°c gi√° tr·ªã TN, FP, FN, TP v√† hi·ªÉn th·ªã tr·ª±c ti·∫øp l√™n bi·ªÉu ƒë·ªì.

### 1.6 `HGTEvaluator.plot_embeddings_tsne`

```python
    def plot_embeddings_tsne(self, save_path=None):
        """Tr·ª±c quan h√≥a embeddings c·ªßa node b·∫±ng t-SNE"""
        embeddings = self.get_embeddings()

        # Load d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω cho nh√£n
        df = pd.read_csv(f"{config.PROCESSED_DATA_PATH}jobs_processed.csv")

        fig, axes = plt.subplots(1, 2, figsize=(16, 7))

        # Bi·ªÉu ƒë·ªì 1: Embeddings c√¥ng vi·ªác theo danh m·ª•c
        job_embeddings = embeddings['job']

        print("ƒêang t√≠nh to√°n t-SNE cho embeddings c√¥ng vi·ªác...")
        tsne = TSNE(n_components=2, random_state=42, perplexity=30)
        job_2d = tsne.fit_transform(job_embeddings)

        # Tr√≠ch xu·∫•t danh m·ª•c c√¥ng vi·ªác t·ª´ ti√™u ƒë·ªÅ
        def extract_category(title):
            title_lower = str(title).lower()
            # ƒê·ªãnh nghƒ©a c√°c danh m·ª•c
            if any(x in title_lower for x in ['developer', 'l·∫≠p tr√¨nh', 'it', 'software', 'backend', 'frontend', 'fullstack', 'k·ªπ s∆∞', 'engineer']):
                return 'IT/Developer'
            elif any(x in title_lower for x in ['k·∫ø to√°n', 'accountant', 'tax', 'thu·∫ø', 'finance', 't√†i ch√≠nh']):
                return 'Accounting/Finance'
            elif any(x in title_lower for x in ['sale', 'b√°n h√†ng', 'kinh doanh', 'marketing', 'market']):
                return 'Sales/Marketing'
            elif any(x in title_lower for x in ['hr', 'nh√¢n s·ª±', 'talent', 'recruitment', 'tuy·ªÉn d·ª•ng']):
                return 'HR/Recruitment'
            elif any(x in title_lower for x in ['thi·∫øt k·∫ø', 'design', 'ux', 'ui', 'ƒë·ªì h·ªça']):
                return 'Design'
            elif any(x in title_lower for x in ['t∆∞ v·∫•n', 'consultant', 'advisor', 'c·ªë v·∫•n']):
                return 'Consulting'
            elif any(x in title_lower for x in ['qu·∫£n l√Ω', 'manager', 'tr∆∞·ªüng ph√≤ng', 'gi√°m ƒë·ªëc', 'director']):
                return 'Management'
            elif any(x in title_lower for x in ['nh√¢n vi√™n', 'staff', 'chuy√™n vi√™n', 'specialist']):
                return 'Staff/Specialist'
            else:
                return 'Other'

        job_categories = df['Title'].apply(extract_category).values
        unique_categories = np.unique(job_categories)
        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_categories)))

        for i, category in enumerate(unique_categories):
            mask = job_categories == category
            axes[0].scatter(job_2d[mask, 0], job_2d[mask, 1],
                          c=[colors[i]], label=category, alpha=0.6, s=50)

        axes[0].set_xlabel('Chi·ªÅu t-SNE 1', fontsize=12, fontweight='bold')
        axes[0].set_ylabel('Chi·ªÅu t-SNE 2', fontsize=12, fontweight='bold')
        axes[0].set_title('Embeddings C√¥ng vi·ªác (theo Danh m·ª•c)', fontsize=14, fontweight='bold')
        axes[0].legend(loc='best', fontsize=9)
        axes[0].grid(True, alpha=0.3)

        # Bi·ªÉu ƒë·ªì 2: Embeddings c√¥ng vi·ªác theo m·ª©c l∆∞∆°ng
        salary_avg = (df['salary_min'] + df['salary_max']) / 2
        scatter = axes[1].scatter(job_2d[:, 0], job_2d[:, 1],
                                 c=salary_avg, cmap='viridis', alpha=0.6, s=50)

        axes[1].set_xlabel('Chi·ªÅu t-SNE 1', fontsize=12, fontweight='bold')
        axes[1].set_ylabel('Chi·ªÅu t-SNE 2', fontsize=12, fontweight='bold')
        axes[1].set_title('Embeddings C√¥ng vi·ªác (theo M·ª©c l∆∞∆°ng)', fontsize=14, fontweight='bold')
        cbar = plt.colorbar(scatter, ax=axes[1])
        cbar.set_label('M·ª©c l∆∞∆°ng TB (tri·ªáu VNƒê)', fontsize=10)
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"‚úÖ Tr·ª±c quan h√≥a embeddings ƒë√£ l∆∞u t·∫°i {save_path}")

        plt.close()

        # In th·ªëng k√™ danh m·ª•c
        print("\nüìä Ph√¢n b·ªë Danh m·ª•c C√¥ng vi·ªác:")
        category_counts = pd.Series(job_categories).value_counts()
        for cat, count in category_counts.items():
            percentage = (count / len(job_categories)) * 100
            print(f"   {cat:20s}: {count:3d} c√¥ng vi·ªác ({percentage:5.1f}%)")
        print(f"   {'='*40}")
        print(f"   {'T·ªïng':20s}: {len(job_categories):3d} c√¥ng vi·ªác")

        return job_categories
```

- **M·ª•c ƒë√≠ch:** S·ª≠ d·ª•ng thu·∫≠t to√°n t-SNE (t-Distributed Stochastic Neighbor Embedding) ƒë·ªÉ gi·∫£m chi·ªÅu d·ªØ li·ªáu embedding (t·ª´ 128 chi·ªÅu xu·ªëng 2 chi·ªÅu) nh·∫±m m·ª•c ƒë√≠ch hi·ªÉn th·ªã l√™n m·∫∑t ph·∫≥ng.
- **Gi·∫£i th√≠ch chi ti·∫øt:**
  - **Data Science Logic:** t-SNE l√† thu·∫≠t to√°n m·∫°nh m·∫Ω ƒë·ªÉ tr·ª±c quan h√≥a d·ªØ li·ªáu cao chi·ªÅu, gi√∫p ta th·∫•y ƒë∆∞·ª£c li·ªáu c√°c ƒëi·ªÉm d·ªØ li·ªáu gi·ªëng nhau c√≥ ƒë·ª©ng g·∫ßn nhau trong kh√¥ng gian vector hay kh√¥ng.
  - **IO:** ƒê·ªçc file CSV `jobs_processed.csv` ƒë·ªÉ l·∫•y th√¥ng tin nh√£n (Title, Salary).
  - **H√†m `extract_category`:** Ph√¢n lo·∫°i c√¥ng vi·ªác th·ªß c√¥ng d·ª±a tr√™n t·ª´ kh√≥a trong ti√™u ƒë·ªÅ (v√≠ d·ª•: 'java' -> 'IT/Developer') ƒë·ªÉ t√¥ m√†u cho c√°c ƒëi·ªÉm tr√™n bi·ªÉu ƒë·ªì.
  - **Bi·ªÉu ƒë·ªì 1:** T√¥ m√†u theo Danh m·ª•c c√¥ng vi·ªác. Ta k·ª≥ v·ªçng c√°c c√¥ng vi·ªác c√πng ng√†nh s·∫Ω c·ª•m l·∫°i v·ªõi nhau.
  - **Bi·ªÉu ƒë·ªì 2:** T√¥ m√†u theo M·ª©c l∆∞∆°ng trung b√¨nh.
  - **K·∫øt qu·∫£:** Bi·ªÉu ƒë·ªì gi√∫p x√°c nh·∫≠n tr·ª±c quan r·∫±ng m√¥ h√¨nh ƒë√£ h·ªçc ƒë∆∞·ª£c ng·ªØ nghƒ©a c·ªßa d·ªØ li·ªáu hay ch∆∞a.

### 1.7 `HGTEvaluator.analyze_recommendations`

```python
    def analyze_recommendations(self, job_idx=0, top_k=10, save_path=None):
        """Ph√¢n t√≠ch top-K g·ª£i √Ω cho m·ªôt c√¥ng vi·ªác c·ª• th·ªÉ"""
        embeddings = self.get_embeddings()
        job_embeddings = embeddings['job']

        # T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng
        target_emb = job_embeddings[job_idx]
        similarities = np.dot(job_embeddings, target_emb) / (
            np.linalg.norm(job_embeddings, axis=1) * np.linalg.norm(target_emb)
        )

        # L·∫•y top-K c√¥ng vi·ªác t∆∞∆°ng ƒë·ªìng nh·∫•t (kh√¥ng bao g·ªìm ch√≠nh n√≥)
        similarities[job_idx] = -1
        top_indices = np.argsort(similarities)[-top_k:][::-1]

        # Load d·ªØ li·ªáu c√¥ng vi·ªác
        df = pd.read_csv(f"{config.PROCESSED_DATA_PATH}jobs_processed.csv")

        # T·∫°o bi·ªÉu ƒë·ªì tr·ª±c quan h√≥a
        fig, axes = plt.subplots(2, 1, figsize=(14, 10))

        # Bi·ªÉu ƒë·ªì tr√™n: Bi·ªÉu ƒë·ªì c·ªôt ƒë·ªô t∆∞∆°ng ƒë·ªìng
        top_sims = similarities[top_indices]
        job_titles = [df.iloc[i]['Title'][:30] for i in top_indices]

        axes[0].barh(range(top_k), top_sims, color='steelblue')
        axes[0].set_yticks(range(top_k))
        axes[0].set_yticklabels(job_titles, fontsize=9)
        axes[0].set_xlabel('ƒê·ªô t∆∞∆°ng ƒë·ªìng Cosine', fontsize=12, fontweight='bold')
        axes[0].set_title(f'Top-{top_k} C√¥ng vi·ªác T∆∞∆°ng ƒë·ªìng v·ªõi: {df.iloc[job_idx]["Title"][:50]}',
                         fontsize=13, fontweight='bold')
        axes[0].invert_yaxis()
        axes[0].grid(True, alpha=0.3, axis='x')

        # Bi·ªÉu ƒë·ªì d∆∞·ªõi: So s√°nh ƒë·∫∑c tr∆∞ng
        target_job = df.iloc[job_idx]
        comparison_data = {
            'C√¥ng vi·ªác ƒê√≠ch': [
                target_job['salary_min'],
                target_job['salary_max'],
                target_job['experience_years'],
                target_job['quantity']
            ]
        }

        for i, idx in enumerate(top_indices[:5]):  # Hi·ªÉn th·ªã top 5
            job = df.iloc[idx]
            comparison_data[f'T∆∞∆°ng ƒë·ªìng #{i+1}'] = [
                job['salary_min'],
                job['salary_max'],
                job['experience_years'],
                job['quantity']
            ]

        comparison_df = pd.DataFrame(comparison_data,
                                     index=['L∆∞∆°ng T·ªëi thi·ªÉu', 'L∆∞∆°ng T·ªëi ƒëa', 'Kinh nghi·ªám (nƒÉm)', 'S·ªë l∆∞·ª£ng'])

        x = np.arange(len(comparison_df.index))
        width = 0.15

        for i, col in enumerate(comparison_df.columns):
            offset = width * (i - len(comparison_df.columns)/2 + 0.5)
            axes[1].bar(x + offset, comparison_df[col], width, label=col)

        axes[1].set_xlabel('ƒê·∫∑c tr∆∞ng', fontsize=12, fontweight='bold')
        axes[1].set_ylabel('Gi√° tr·ªã', fontsize=12, fontweight='bold')
        axes[1].set_title('So s√°nh ƒê·∫∑c tr∆∞ng', fontsize=13, fontweight='bold')
        axes[1].set_xticks(x)
        axes[1].set_xticklabels(comparison_df.index)
        axes[1].legend(loc='upper right', fontsize=9)
        axes[1].grid(True, alpha=0.3, axis='y')

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"‚úÖ Ph√¢n t√≠ch g·ª£i √Ω ƒë√£ l∆∞u t·∫°i {save_path}")

        plt.close()

        return top_indices, similarities[top_indices]
```

- **M·ª•c ƒë√≠ch:** Ki·ªÉm th·ª≠ th·ª±c t·∫ø kh·∫£ nƒÉng g·ª£i √Ω c·ªßa m√¥ h√¨nh b·∫±ng c√°ch ch·ªçn 1 c√¥ng vi·ªác b·∫•t k·ª≥ v√† t√¨m ra c√°c c√¥ng vi·ªác t∆∞∆°ng t·ª± nh·∫•t.
- **Gi·∫£i th√≠ch chi ti·∫øt:**
  - **Thu·∫≠t to√°n Cosine Similarity:**
    - C√¥ng th·ª©c: `dot(A, B) / (norm(A) * norm(B))`.
    - D√πng ƒë·ªÉ ƒëo g√≥c gi·ªØa hai vector embedding. G√≥c c√†ng nh·ªè (gi√° tr·ªã c√†ng g·∫ßn 1) th√¨ c√†ng t∆∞∆°ng ƒë·ªìng.
  - **Logic:**
    1.  T√≠nh Cosine Similarity gi·ªØa c√¥ng vi·ªác m·ª•c ti√™u (`job_idx`) v√† T·∫§T C·∫¢ c√°c c√¥ng vi·ªác kh√°c.
    2.  S·∫Øp x·∫øp (`argsort`) v√† l·∫•y ra `top_k` ch·ªâ s·ªë c√≥ ƒëi·ªÉm cao nh·∫•t.
    3.  Hi·ªÉn th·ªã t√™n c√°c c√¥ng vi·ªác g·ª£i √Ω.
    4.  So s√°nh c√°c th√¥ng s·ªë (L∆∞∆°ng, Kinh nghi·ªám) gi·ªØa c√¥ng vi·ªác g·ªëc v√† c√¥ng vi·ªác g·ª£i √Ω ƒë·ªÉ xem li·ªáu g·ª£i √Ω c√≥ h·ª£p l√Ω v·ªÅ m·∫∑t logic nghi·ªáp v·ª• kh√¥ng.

### 1.8 `main` (hgt_evaluation.py)

```python
def main():
    """ƒê√°nh gi√° to√†n di·ªán"""
    print("\n" + "="*70)
    print(" "*15 + "ƒê√ÅNH GI√Å & TR·ª∞C QUAN H√ìA M√î H√åNH HGT")
    print("="*70)

    # T·∫£i ƒë·ªì th·ªã v√† d·ªØ li·ªáu
    print("\n[1/6] ƒêang t·∫£i d·ªØ li·ªáu...")
    graph = torch.load(f"{config.GRAPH_DATA_PATH}hetero_graph.pt")

    # T·∫£i d·ªØ li·ªáu test (c·∫ßn t·∫°o l·∫°i split)
    from torch_geometric.transforms import RandomLinkSplit
    edge_type = ('job', 'similar_to', 'job')
    transform = RandomLinkSplit(
        num_val=0.1,
        num_test=0.1,
        edge_types=[edge_type],
        rev_edge_types=[edge_type],
        add_negative_train_samples=True,
        neg_sampling_ratio=1.0,
    )
    _, _, test_data = transform(graph)
    print("‚úÖ D·ªØ li·ªáu ƒë√£ t·∫£i")

    # T·∫£i m√¥ h√¨nh
    print("\n[2/6] ƒêang t·∫£i m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán...")
    from hgt_model import create_hgt_model
    model = create_hgt_model(graph, task='link_prediction',
                            hidden_channels=128, out_channels=64,
                            num_heads=8, num_layers=2)

    checkpoint = torch.load(f"{config.GRAPH_DATA_PATH}best_model.pt", map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    print("‚úÖ M√¥ h√¨nh ƒë√£ t·∫£i")

    # T·∫°o evaluator
    print("\n[3/6] ƒêang t·∫°o evaluator...")
    evaluator = HGTEvaluator(model, graph, test_data, edge_type, device='cpu')
    print("‚úÖ Evaluator ƒë√£ t·∫°o")

    # T·∫°o c√°c bi·ªÉu ƒë·ªì tr·ª±c quan
    print("\n[4/6] ƒêang t·∫°o ƒë∆∞·ªùng cong ROC & PR...")
    evaluator.plot_roc_pr_curves(save_path=f"{config.GRAPH_DATA_PATH}hgt_roc_pr_curves.png")

    print("\n[5/6] ƒêang t·∫°o ma tr·∫≠n nh·∫ßm l·∫´n...")
    evaluator.plot_confusion_matrix(save_path=f"{config.GRAPH_DATA_PATH}hgt_confusion_matrix.png")

    print("\n[6/6] ƒêang t·∫°o tr·ª±c quan embeddings...")
    evaluator.plot_embeddings_tsne(save_path=f"{config.GRAPH_DATA_PATH}hgt_embeddings_tsne.png")

    print("\n[7/7] ƒêang ph√¢n t√≠ch g·ª£i √Ω...")
    evaluator.analyze_recommendations(job_idx=0, top_k=10,
                                     save_path=f"{config.GRAPH_DATA_PATH}hgt_recommendations.png")

    print("\n" + "="*70)
    print(" "*20 + "üéâ ƒê√ÅNH GI√Å HO√ÄN T·∫§T! üéâ")
    print("="*70)
    print(f"\nC√°c bi·ªÉu ƒë·ªì ƒë√£ t·∫°o:")
    print(f"  üìä {config.GRAPH_DATA_PATH}hgt_roc_pr_curves.png")
    print(f"  üìä {config.GRAPH_DATA_PATH}hgt_confusion_matrix.png")
    print(f"  üìä {config.GRAPH_DATA_PATH}hgt_embeddings_tsne.png")
    print(f"  üìä {config.GRAPH_DATA_PATH}hgt_recommendations.png")
    print("\n")
```

- **M·ª•c ƒë√≠ch:** File th·ª±c thi ch√≠nh (Entry Point) cho qu√° tr√¨nh ƒë√°nh gi√°.
- **Lu·ªìng x·ª≠ l√Ω:**
  1.  Load file ƒë·ªì th·ªã `hetero_graph.pt`.
  2.  T√°i t·∫°o l·∫°i vi·ªác chia d·ªØ li·ªáu `RandomLinkSplit` ƒë·ªÉ c√≥ t·∫≠p `test_data` gi·ªëng nh∆∞ l√∫c train (l∆∞u √Ω: ƒë·ªÉ ch√≠nh x√°c tuy·ªát ƒë·ªëi, h·∫°t gi·ªëng ng·∫´u nhi√™n (seed) c·∫ßn ph·∫£i gi·ªëng nhau).
  3.  Kh·ªüi t·∫°o ki·∫øn tr√∫c m√¥ h√¨nh HGT (c·∫•u tr√∫c ph·∫£i kh·ªõp v·ªõi l√∫c train).
  4.  Load tr·ªçng s·ªë (weights) t·ª´ file `best_model.pt`.
  5.  Kh·ªüi t·∫°o `HGTEvaluator`.
  6.  G·ªçi l·∫ßn l∆∞·ª£t c√°c h√†m v·∫Ω bi·ªÉu ƒë·ªì.

---

## 2. hgt_model.py

T·ªáp n√†y ƒë·ªãnh nghƒ©a ki·∫øn tr√∫c c·ªët l√µi c·ªßa m·∫°ng n∆°-ron Heterogeneous Graph Transformer. ƒê√¢y l√† "tr√°i tim" c·ªßa h·ªá th·ªëng tr√≠ tu·ªá nh√¢n t·∫°o d·ª± √°n.

### 2.1 `HGT.__init__`

```python
    def __init__(
        self,
        metadata,
        hidden_channels=128,
        out_channels=64,
        num_heads=8,
        num_layers=2,
        node_type_dims=None,
    ):
        """
        Tham s·ªë:
            metadata: Metadata c·ªßa PyG HeteroData (c√°c lo·∫°i node, edge)
            hidden_channels: K√≠ch th∆∞·ªõc chi·ªÅu ·∫©n
            out_channels: K√≠ch th∆∞·ªõc embedding ƒë·∫ßu ra
            num_heads: S·ªë l∆∞·ª£ng attention heads
            num_layers: S·ªë l∆∞·ª£ng l·ªõp HGT
            node_type_dims: Dict √°nh x·∫° lo·∫°i node ƒë·∫øn k√≠ch th∆∞·ªõc features ƒë·∫ßu v√†o
        """
        super().__init__()

        self.metadata = metadata
        self.hidden_channels = hidden_channels
        self.out_channels = out_channels
        self.num_heads = num_heads
        self.num_layers = num_layers

        # L·ªõp projection ƒë·∫ßu v√†o cho m·ªói lo·∫°i node
        self.lin_dict = nn.ModuleDict()
        for node_type, dim in node_type_dims.items():
            self.lin_dict[node_type] = Linear(dim, hidden_channels)

        # C√°c l·ªõp HGT Convolution
        self.convs = nn.ModuleList()
        for _ in range(num_layers):
            conv = HGTConv(
                hidden_channels,
                hidden_channels,
                metadata,
                num_heads,
            )
            self.convs.append(conv)

        # L·ªõp projection ƒë·∫ßu ra
        self.lin_out = Linear(hidden_channels, out_channels)
```

- **M·ª•c ƒë√≠ch:** Kh·ªüi t·∫°o ki·∫øn tr√∫c m·∫°ng HGT.
- **Gi·∫£i th√≠ch chi ti·∫øt (OOP/Deep Learning):**
  - K·∫ø th·ª´a t·ª´ `nn.Module` (l·ªõp c∆° s·ªü c·ªßa m·ªçi m·∫°ng n∆°-ron trong PyTorch).
  - **Input Projection (`lin_dict`):** V√¨ c√°c lo·∫°i node kh√°c nhau (job, company, location) c√≥ s·ªë l∆∞·ª£ng features ƒë·∫ßu v√†o kh√°c nhau, ch√∫ng ta c·∫ßn c√°c l·ªõp Linear ri√™ng bi·ªát ƒë·ªÉ chi·∫øu t·∫•t c·∫£ v·ªÅ c√πng m·ªôt kh√¥ng gian vector `hidden_channels` tr∆∞·ªõc khi ƒë∆∞a v√†o HGT.
  - **HGT Layers (`convs`):** S·ª≠ d·ª•ng `HGTConv` t·ª´ th∆∞ vi·ªán `torch_geometric`. ƒê√¢y l√† l·ªõp th·ª±c hi·ªán c∆° ch·∫ø Attention tr√™n ƒë·ªì th·ªã d·ªã th·ªÉ.
  - **Output Projection (`lin_out`):** L·ªõp Linear cu·ªëi c√πng ƒë·ªÉ ƒë∆∞a vector v·ªÅ k√≠ch th∆∞·ªõc mong mu·ªën `out_channels`.

### 2.2 `HGT.forward`

```python
    def forward(self, x_dict, edge_index_dict):
        """
        Lan truy·ªÅn xu√¥i (Forward pass)

        Tham s·ªë:
            x_dict: Dictionary ch·ª©a features c·ªßa c√°c node {lo·∫°i_node: features}
            edge_index_dict: Dictionary ch·ª©a ch·ªâ s·ªë c√°c c·∫°nh {lo·∫°i_edge: edge_index}

        Tr·∫£ v·ªÅ:
            Dictionary ch·ª©a embeddings c·ªßa c√°c node {lo·∫°i_node: embeddings}
        """
        # Projection ƒë·∫ßu v√†o
        x_dict = {
            node_type: self.lin_dict[node_type](x).relu()
            for node_type, x in x_dict.items()
        }

        # C√°c l·ªõp HGT convolution
        for conv in self.convs:
            x_dict = conv(x_dict, edge_index_dict)
            x_dict = {key: x.relu() for key, x in x_dict.items()}

        # Projection ƒë·∫ßu ra
        out_dict = {
            node_type: self.lin_out(x)
            for node_type, x in x_dict.items()
        }

        return out_dict
```

- **M·ª•c ƒë√≠ch:** ƒê·ªãnh nghƒ©a lu·ªìng d·ªØ li·ªáu ƒëi qua m·∫°ng.
- **Lu·ªìng x·ª≠ l√Ω:**
  1.  **Input Linear:** Features th√¥ -> Linear -> ReLU -> Hidden Features.
  2.  **HGT Message Passing:** L·∫∑p qua s·ªë l·ªõp `num_layers`. M·ªói l·ªõp `conv` s·∫Ω t·ªïng h·ª£p th√¥ng tin t·ª´ c√°c node l√¢n c·∫≠n d·ª±a tr√™n metadata c·ªßa ƒë·ªì th·ªã. Sau m·ªói l·ªõp ƒë·ªÅu d√πng h√†m k√≠ch ho·∫°t `relu`.
  3.  **Output Linear:** Hidden Features -> Linear -> Output Embeddings.
  4.  Tr·∫£ v·ªÅ dictionary ch·ª©a embeddings m·ªõi cho t·ª´ng lo·∫°i node.

### 2.3 `HGTLinkPredictor` (C√°c methods)

L·ªõp n√†y bao b·ªçc l·ªõp `HGT` ƒë·ªÉ th·ª±c hi·ªán nhi·ªám v·ª• c·ª• th·ªÉ l√† d·ª± ƒëo√°n li√™n k·∫øt.

#### `__init__`

```python
    def __init__(
        self,
        metadata,
        hidden_channels=128,
        out_channels=64,
        num_heads=8,
        num_layers=2,
        node_type_dims=None,
    ):
        super().__init__()

        # HGT encoder
        self.hgt = HGT(
            metadata,
            hidden_channels,
            out_channels,
            num_heads,
            num_layers,
            node_type_dims,
        )

        # ƒê·∫ßu d·ª± ƒëo√°n li√™n k·∫øt
        self.predictor = nn.Sequential(
            nn.Linear(out_channels * 2, hidden_channels),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_channels, 1)
        )
```

- **Gi·∫£i th√≠ch:**
  - Ch·ª©a th√†nh ph·∫ßn `self.hgt` ƒë·ªÉ t·∫°o embeddings.
  - Ch·ª©a th√†nh ph·∫ßn `self.predictor`: ƒê√¢y l√† m·ªôt m·∫°ng MLP (Multi-Layer Perceptron) nh·ªè. ƒê·∫ßu v√†o l√† `out_channels * 2` v√¨ n√≥ s·∫Ω gh√©p n·ªëi (concat) embedding c·ªßa 2 node l·∫°i v·ªõi nhau ƒë·ªÉ d·ª± ƒëo√°n m·ªëi quan h·ªá.

#### `forward`

```python
    def forward(self, x_dict, edge_index_dict, edge_label_index, edge_type):
        """
        Tham s·ªë:
            x_dict: Features c·ªßa c√°c node
            edge_index_dict: Ch·ªâ s·ªë c√°c c·∫°nh
            edge_label_index: C√°c c·∫°nh c·∫ßn d·ª± ƒëo√°n (2, num_edges)
            edge_type: Lo·∫°i c·∫°nh c·∫ßn d·ª± ƒëo√°n ('job', 'similar_to', 'job')

        Tr·∫£ v·ªÅ:
            ƒêi·ªÉm s·ªë d·ª± ƒëo√°n li√™n k·∫øt
        """
        # L·∫•y embeddings c·ªßa c√°c node
        node_emb_dict = self.hgt(x_dict, edge_index_dict)

        # L·∫•y lo·∫°i node ngu·ªìn v√† ƒë√≠ch
        src_type, _, dst_type = edge_type

        # L·∫•y embeddings cho c√°c c·∫°nh c·∫ßn d·ª± ƒëo√°n
        src_emb = node_emb_dict[src_type][edge_label_index[0]]
        dst_emb = node_emb_dict[dst_type][edge_label_index[1]]

        # Gh√©p n·ªëi v√† d·ª± ƒëo√°n
        edge_emb = torch.cat([src_emb, dst_emb], dim=-1)
        pred = self.predictor(edge_emb).squeeze(-1)

        return pred
```

- **Lu·ªìng x·ª≠ l√Ω:**
  1.  Ch·∫°y `self.hgt` ƒë·ªÉ l·∫•y vector ƒë·∫∑c tr∆∞ng cho T·∫§T C·∫¢ c√°c node trong ƒë·ªì th·ªã.
  2.  `edge_label_index` ch·ª©a c√°c c·∫∑p node c·∫ßn ki·ªÉm tra (v√≠ d·ª•: Job A - Job B).
  3.  L·∫•y vector c·ªßa Job A (`src_emb`) v√† vector c·ªßa Job B (`dst_emb`).
  4.  `torch.cat`: N·ªëi ƒëu√¥i hai vector n√†y l·∫°i.
  5.  ƒê∆∞a qua `predictor` ƒë·ªÉ t√≠nh ra m·ªôt ƒëi·ªÉm s·ªë (score) duy nh·∫•t th·ªÉ hi·ªán kh·∫£ nƒÉng t·ªìn t·∫°i li√™n k·∫øt.

#### `encode`

```python
    def encode(self, x_dict, edge_index_dict):
        """L·∫•y embeddings c·ªßa c√°c node"""
        return self.hgt(x_dict, edge_index_dict)
```

- **M·ª•c ƒë√≠ch:** H√†m wrapper ti·ªán √≠ch ƒë·ªÉ ch·ªâ l·∫•y embeddings m√† kh√¥ng c·∫ßn d·ª± ƒëo√°n.

### 2.4 `HGTNodeClassifier` (C√°c methods)

L·ªõp n√†y bao b·ªçc `HGT` cho nhi·ªám v·ª• ph√¢n lo·∫°i node (v√≠ d·ª•: ph√¢n lo·∫°i m·ª©c l∆∞∆°ng c·ªßa c√¥ng vi·ªác).

#### `__init__`

```python
    def __init__(
        self,
        metadata,
        num_classes,
        hidden_channels=128,
        out_channels=64,
        num_heads=8,
        num_layers=2,
        node_type_dims=None,
        target_node_type='job',
    ):
        super().__init__()

        self.target_node_type = target_node_type

        # HGT encoder
        self.hgt = HGT(
            metadata,
            hidden_channels,
            out_channels,
            num_heads,
            num_layers,
            node_type_dims,
        )

        # ƒê·∫ßu ph√¢n lo·∫°i
        self.classifier = nn.Sequential(
            nn.Linear(out_channels, hidden_channels),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_channels, num_classes)
        )
```

- **Gi·∫£i th√≠ch:**
  - T∆∞∆°ng t·ª± `LinkPredictor` nh∆∞ng `classifier` ƒë·∫ßu ra c√≥ k√≠ch th∆∞·ªõc l√† `num_classes` (s·ªë l∆∞·ª£ng l·ªõp ph√¢n lo·∫°i, v√≠ d·ª•: th·∫•p/trung b√¨nh/cao).

#### `forward`

```python
    def forward(self, x_dict, edge_index_dict):
        """
        Tham s·ªë:
            x_dict: Features c·ªßa c√°c node
            edge_index_dict: Ch·ªâ s·ªë c√°c c·∫°nh

        Tr·∫£ v·ªÅ:
            Logits ph√¢n lo·∫°i cho lo·∫°i node ƒë√≠ch
        """
        # L·∫•y embeddings c·ªßa c√°c node
        node_emb_dict = self.hgt(x_dict, edge_index_dict)

        # Ph√¢n lo·∫°i c√°c node ƒë√≠ch
        target_emb = node_emb_dict[self.target_node_type]
        logits = self.classifier(target_emb)

        return logits
```

- **Lu·ªìng x·ª≠ l√Ω:** L·∫•y embedding c·ªßa node m·ª•c ti√™u -> ƒë∆∞a qua l·ªõp ph√¢n lo·∫°i -> tr·∫£ v·ªÅ x√°c su·∫•t thu·ªôc v·ªÅ t·ª´ng l·ªõp.

### 2.5 `create_hgt_model`

```python
def create_hgt_model(graph, task='link_prediction', **kwargs):
    """
    H√†m factory ƒë·ªÉ t·∫°o m√¥ h√¨nh HGT

    Tham s·ªë:
        graph: ƒê·ªëi t∆∞·ª£ng PyG HeteroData
        task: 'link_prediction' ho·∫∑c 'node_classification'
        **kwargs: C√°c tham s·ªë m√¥ h√¨nh b·ªï sung

    Tr·∫£ v·ªÅ:
        Instance c·ªßa m√¥ h√¨nh HGT
    """
    # L·∫•y metadata
    metadata = graph.metadata()

    # L·∫•y k√≠ch th∆∞·ªõc c·ªßa c√°c lo·∫°i node
    node_type_dims = {
        'job': graph['job'].x.shape[1],
        'company': graph['company'].x.shape[1],
        'location': graph['location'].x.shape[1],
    }

    # Tham s·ªë m·∫∑c ƒë·ªãnh
    default_params = {
        'hidden_channels': 128,
        'out_channels': 64,
        'num_heads': 8,
        'num_layers': 2,
    }
    default_params.update(kwargs)

    # T·∫°o m√¥ h√¨nh d·ª±a tr√™n t√°c v·ª•
    if task == 'link_prediction':
        model = HGTLinkPredictor(
            metadata=metadata,
            node_type_dims=node_type_dims,
            **default_params
        )
    elif task == 'node_classification':
        if 'num_classes' not in kwargs:
            raise ValueError("num_classes required for node classification")
        model = HGTNodeClassifier(
            metadata=metadata,
            node_type_dims=node_type_dims,
            **default_params
        )
    else:
        raise ValueError(f"Unknown task: {task}")

    return model
```

- **M·ª•c ƒë√≠ch:** √Åp d·ª•ng m·∫´u thi·∫øt k·∫ø **Factory Pattern**. Thay v√¨ kh·ªüi t·∫°o class tr·ª±c ti·∫øp, ta d√πng h√†m n√†y ƒë·ªÉ t·ª± ƒë·ªông tr√≠ch xu·∫•t metadata t·ª´ graph v√† t·∫°o ƒë√∫ng lo·∫°i class (LinkPredictor ho·∫∑c NodeClassifier) d·ª±a tr√™n tham s·ªë `task`.

---

## 3. hgt_summary.py

T·ªáp ti·ªán √≠ch ƒë∆°n gi·∫£n ƒë·ªÉ in ra b√°o c√°o t·ªïng k·∫øt sau khi ch·∫°y hu·∫•n luy·ªán, gi√∫p ng∆∞·ªùi d√πng n·∫Øm b·∫Øt nhanh tr·∫°ng th√°i h·ªá th·ªëng.

### 3.1 `print_section`

```python
def print_section(title, width=70):
    """In ti√™u ƒë·ªÅ ph·∫ßn c√≥ ƒë·ªãnh d·∫°ng"""
    print("\n" + "="*width)
    print(f" {title.center(width-2)} ")
    print("="*width)
```

- **M·ª•c ƒë√≠ch:** H√†m h·ªó tr·ª£ ƒë·ªãnh d·∫°ng chu·ªói, in ra c√°c ti√™u ƒë·ªÅ ƒë∆∞·ª£c cƒÉn gi·ªØa v·ªõi ƒë∆∞·ªùng vi·ªÅn d·∫•u b·∫±ng, gi√∫p log file d·ªÖ ƒë·ªçc h∆°n.

### 3.2 `summarize_results`

```python
def summarize_results():
    """T·∫°o b√°o c√°o t·ªïng h·ª£p v·ªÅ c√°c th√≠ nghi·ªám HGT"""

    print_section("T·ªîNG H·ª¢P TH√ç NGHI·ªÜM HGT")
    print(f"\nT·∫°o l√∫c: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # 1. Th√¥ng tin M√¥ h√¨nh
    # ... (Code ki·ªÉm tra file best_model.pt v√† in k√≠ch th∆∞·ªõc)

    # 2. Th√¥ng tin ƒê·ªì th·ªã
    # ... (Code load graph v√† in s·ªë l∆∞·ª£ng node/edge)

    # 3. C·∫•u h√¨nh Hu·∫•n luy·ªán
    # ... (In hardcode c√°c tham s·ªë config)

    # 4. C√°c Bi·ªÉu ƒë·ªì ƒê√£ t·∫°o
    # ... (Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa c√°c file png)

    # 5. C√°c File ƒê·∫ßu ra
    # ... (Li·ªát k√™ ƒë∆∞·ªùng d·∫´n file)

    # 6. H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng
    # ... (In h∆∞·ªõng d·∫´n command line)

    # Footer
    # ...
```

- **M·ª•c ƒë√≠ch:** Cung c·∫•p c√°i nh√¨n to√†n c·∫£nh (Dashboard d·∫°ng text) v·ªÅ project.
- **IO:** Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa file (`os.path.exists`), l·∫•y k√≠ch th∆∞·ªõc file, ƒë·ªçc file `.pt` ƒë·ªÉ l·∫•y metadata.
- **Logic:** Tu·∫ßn t·ª± ki·ªÉm tra t·ª´ng th√†nh ph·∫ßn (Model, Graph, Plot) v√† in tr·∫°ng th√°i (‚úÖ ho·∫∑c ‚ùå).

---

## 4. train_hgt.py

T·ªáp n√†y qu·∫£n l√Ω quy tr√¨nh hu·∫•n luy·ªán (Training Loop) cho m√¥ h√¨nh.

### 4.1 `HGTTrainer.__init__`

```python
    def __init__(
        self,
        model,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        lr=0.001,
        weight_decay=1e-5,
    ):
        """
        Tham s·ªë:
            model: M√¥ h√¨nh HGT
            device: Thi·∫øt b·ªã ƒë·ªÉ hu·∫•n luy·ªán
            lr: T·ªëc ƒë·ªô h·ªçc (learning rate)
            weight_decay: H·ªá s·ªë suy gi·∫£m tr·ªçng s·ªë
        """
        self.model = model.to(device)
        self.device = device

        self.optimizer = torch.optim.Adam(
            model.parameters(), lr=lr, weight_decay=weight_decay
        )

        print(f"\n{'='*60}")
        print(f"Kh·ªüi t·∫°o HGT Trainer")
        print(f"{'='*60}")
        print(f"Thi·∫øt b·ªã: {device}")
        print(f"T·ªëc ƒë·ªô h·ªçc: {lr}")
        print(f"H·ªá s·ªë suy gi·∫£m: {weight_decay}")
        print(f"S·ªë tham s·ªë m√¥ h√¨nh: {sum(p.numel() for p in model.parameters()):,}")
```

- **M·ª•c ƒë√≠ch:** Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng hu·∫•n luy·ªán.
- **Logic:**
  - Chuy·ªÉn model sang GPU/CPU.
  - Kh·ªüi t·∫°o `Adam Optimizer`: Thu·∫≠t to√°n t·ªëi ∆∞u h√≥a ph·ªï bi·∫øn nh·∫•t cho Deep Learning hi·ªán nay. N√≥ s·∫Ω c·∫≠p nh·∫≠t tr·ªçng s·ªë c·ªßa model ƒë·ªÉ gi·∫£m thi·ªÉu h√†m m·∫•t m√°t.

### 4.2 `HGTTrainer.train_epoch`

```python
    def train_epoch(self, data, edge_type):
        """Hu·∫•n luy·ªán m·ªôt epoch"""
        self.model.train()
        self.optimizer.zero_grad()

        data = data.to(self.device)

        # L·∫•y c√°c c·∫°nh d∆∞∆°ng v√† √¢m
        edge_label_index = data[edge_type].edge_label_index
        edge_label = data[edge_type].edge_label

        # Lan truy·ªÅn xu√¥i
        x_dict = {
            'job': data['job'].x,
            'company': data['company'].x,
            'location': data['location'].x,
        }

        edge_index_dict = {
            key: data[key].edge_index
            for key in data.edge_types
        }

        pred = self.model(x_dict, edge_index_dict, edge_label_index, edge_type)

        # H√†m m·∫•t m√°t binary cross entropy
        loss = F.binary_cross_entropy_with_logits(pred, edge_label.float())

        loss.backward()
        self.optimizer.step()

        return loss.item()
```

- **M·ª•c ƒë√≠ch:** Th·ª±c hi·ªán 1 v√≤ng l·∫∑p hu·∫•n luy·ªán (1 epoch).
- **Quy tr√¨nh chu·∫©n trong PyTorch:**
  1.  `model.train()`: B·∫≠t ch·∫ø ƒë·ªô training (quan tr·ªçng cho Dropout/BatchNorm).
  2.  `optimizer.zero_grad()`: X√≥a s·∫°ch c√°c gradient c≈© ƒë·ªÉ kh√¥ng b·ªã c·ªông d·ªìn.
  3.  **Forward Pass:** Ch·∫°y d·ªØ li·ªáu qua model ƒë·ªÉ l·∫•y d·ª± ƒëo√°n `pred`.
  4.  **Loss Calculation:** T√≠nh sai s·ªë gi·ªØa d·ª± ƒëo√°n v√† th·ª±c t·∫ø d√πng `binary_cross_entropy_with_logits` (th√≠ch h·ª£p cho b√†i to√°n ph√¢n lo·∫°i nh·ªã ph√¢n nh∆∞ d·ª± ƒëo√°n li√™n k·∫øt).
  5.  **Backward Pass (`loss.backward()`):** T√≠nh ƒë·∫°o h√†m (gradient) ng∆∞·ª£c t·ª´ loss v·ªÅ c√°c tr·ªçng s·ªë (Backpropagation).
  6.  **Optimizer Step (`optimizer.step()`):** C·∫≠p nh·∫≠t tr·ªçng s·ªë d·ª±a tr√™n gradient v·ª´a t√≠nh.

### 4.3 `HGTTrainer.evaluate`

```python
    @torch.no_grad()
    def evaluate(self, data, edge_type):
        """ƒê√°nh gi√° m√¥ h√¨nh"""
        self.model.eval()

        data = data.to(self.device)

        edge_label_index = data[edge_type].edge_label_index
        edge_label = data[edge_type].edge_label

        x_dict = {
            'job': data['job'].x,
            'company': data['company'].x,
            'location': data['location'].x,
        }

        edge_index_dict = {
            key: data[key].edge_index
            for key in data.edge_types
        }

        pred = self.model(x_dict, edge_index_dict, edge_label_index, edge_type)
        pred = torch.sigmoid(pred)

        preds = pred.cpu().numpy()
        labels = edge_label.cpu().numpy()

        # T√≠nh c√°c ch·ªâ s·ªë
        auc = roc_auc_score(labels, preds)
        ap = average_precision_score(labels, preds)

        return auc, ap
```

- **M·ª•c ƒë√≠ch:** Ki·ªÉm tra ƒë·ªô ch√≠nh x√°c c·ªßa model hi·ªán t·∫°i tr√™n t·∫≠p validation ho·∫∑c test.
- **L∆∞u √Ω:** H√†m n√†y t∆∞∆°ng t·ª± h√†m trong `HGTEvaluator` nh∆∞ng ƒë∆°n gi·∫£n h∆°n, ch·ªâ tr·∫£ v·ªÅ ch·ªâ s·ªë AUC v√† AP ƒë·ªÉ ph·ª•c v·ª• vi·ªác theo d√µi qu√° tr√¨nh training.

### 4.4 `HGTTrainer.train`

```python
    def train(
        self,
        train_data,
        val_data,
        test_data,
        edge_type,
        epochs=50,
        eval_every=5,
    ):
        """
        V√≤ng l·∫∑p hu·∫•n luy·ªán ƒë·∫ßy ƒë·ªß
        ...
        """
        print(f"\n{'='*60}")
        print("B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán")
        # ... (In th√¥ng tin)

        best_val_auc = 0
        best_epoch = 0

        for epoch in range(1, epochs + 1):
            # Hu·∫•n luy·ªán
            loss = self.train_epoch(train_data, edge_type)

            # ƒê√°nh gi√°
            if epoch % eval_every == 0:
                train_auc, train_ap = self.evaluate(train_data, edge_type)
                val_auc, val_ap = self.evaluate(val_data, edge_type)

                print(f"\nEpoch {epoch:03d}:")
                print(f"  M·∫•t m√°t: {loss:.4f}")
                print(f"  Hu·∫•n luy·ªán - AUC: {train_auc:.4f}, AP: {train_ap:.4f}")
                print(f"  Validation - AUC: {val_auc:.4f}, AP: {val_ap:.4f}")

                # L∆∞u m√¥ h√¨nh t·ªët nh·∫•t
                if val_auc > best_val_auc:
                    best_val_auc = val_auc
                    best_epoch = epoch
                    self.save_model('best_model.pt')
                    print(f"  ‚úÖ M√¥ h√¨nh t·ªët nh·∫•t ƒë√£ ƒë∆∞·ª£c l∆∞u!")
            else:
                print(f"Epoch {epoch:03d}: M·∫•t m√°t = {loss:.4f}")

        # ... (Load best model v√† test cu·ªëi c√πng)

        return { ... }
```

- **M·ª•c ƒë√≠ch:** ƒêi·ªÅu ph·ªëi to√†n b·ªô qu√° tr√¨nh training qua nhi·ªÅu epochs.
- **Logic:**
  - V√≤ng l·∫∑p ch·∫°y t·ª´ 1 ƒë·∫øn `epochs`.
  - G·ªçi `train_epoch` m·ªói v√≤ng.
  - M·ªói `eval_every` epoch, g·ªçi `evaluate` ƒë·ªÉ ki·ªÉm tra tr√™n t·∫≠p validation.
  - **C∆° ch·∫ø Checkpoint:** N·∫øu AUC tr√™n t·∫≠p validation cao h∆°n k·ª∑ l·ª•c c≈© (`best_val_auc`), l∆∞u model l·∫°i (`save_model`). ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o ta lu√¥n gi·ªØ l·∫°i phi√™n b·∫£n t·ªët nh·∫•t ch·ª© kh√¥ng ph·∫£i phi√™n b·∫£n cu·ªëi c√πng (tr√°nh overfitting).

### 4.5 `HGTTrainer.save_model`, `HGTTrainer.load_model`

```python
    def save_model(self, filename):
        """L∆∞u checkpoint c·ªßa m√¥ h√¨nh"""
        path = os.path.join(config.GRAPH_DATA_PATH, filename)
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, path)

    def load_model(self, filename):
        """T·∫£i checkpoint c·ªßa m√¥ h√¨nh"""
        path = os.path.join(config.GRAPH_DATA_PATH, filename)
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
```

- **IO:** S·ª≠ d·ª•ng `torch.save` v√† `torch.load` ƒë·ªÉ ghi/ƒë·ªçc file nh·ªã ph√¢n.
- **L∆∞u √Ω:** Ta l∆∞u c·∫£ `optimizer_state_dict` ƒë·ªÉ c√≥ th·ªÉ ti·∫øp t·ª•c training t·ª´ ƒëi·ªÉm d·ª´ng n·∫øu c·∫ßn thi·∫øt (resume training).

### 4.6 `prepare_data`

```python
def prepare_data(graph, edge_type=('job', 'similar_to', 'job'), split_ratio=[0.8, 0.1, 0.1]):
    """
    Chu·∫©n b·ªã chia d·ªØ li·ªáu train/val/test
    ...
    """
    # ... (In th√¥ng tin)

    # Chia c√°c c·∫°nh th√†nh train/val/test
    transform = RandomLinkSplit(
        num_val=split_ratio[1],
        num_test=split_ratio[2],
        edge_types=[edge_type],
        rev_edge_types=[edge_type],  # V√¨ c√≥ c·∫°nh hai chi·ªÅu
        add_negative_train_samples=True,
        neg_sampling_ratio=1.0,  # T·ªâ l·ªá m·∫´u d∆∞∆°ng:m·∫´u √¢m = 1:1
    )

    train_data, val_data, test_data = transform(graph)

    # ... (In k·∫øt qu·∫£)

    return train_data, val_data, test_data
```

- **M·ª•c ƒë√≠ch:** Chia t·∫≠p d·ªØ li·ªáu ƒë·ªì th·ªã th√†nh 3 ph·∫ßn.
- **Logic:**
  - S·ª≠ d·ª•ng `RandomLinkSplit` c·ªßa PyG. ƒê√¢y l√† c√¥ng c·ª• chuy√™n d·ª•ng cho b√†i to√°n Link Prediction.
  - N√≥ s·∫Ω gi·∫•u ƒëi m·ªôt s·ªë c·∫°nh trong `edge_index` ƒë·ªÉ l√†m d·ªØ li·ªáu ki·ªÉm th·ª≠ (positive samples).
  - N√≥ c≈©ng t·ª± ƒë·ªông sinh ra c√°c c·∫°nh gi·∫£ (negative samples - n·ªëi 2 node th·ª±c t·∫ø kh√¥ng li√™n k·∫øt v·ªõi nhau) v·ªõi t·ªâ l·ªá 1:1 ƒë·ªÉ model h·ªçc c√°ch ph√¢n bi·ªát li√™n k·∫øt th·∫≠t v√† gi·∫£.

### 4.7 `main` (train_hgt.py)

```python
def main():
    """Pipeline hu·∫•n luy·ªán ch√≠nh"""
    # ... (In ti√™u ƒë·ªÅ)

    # Load ƒë·ªì th·ªã
    # ... (Code load graph file)

    # Chu·∫©n b·ªã d·ªØ li·ªáu
    # ... (G·ªçi prepare_data)

    # T·∫°o m√¥ h√¨nh
    # ... (G·ªçi create_hgt_model)

    # Hu·∫•n luy·ªán
    # ... (Kh·ªüi t·∫°o HGTTrainer v√† g·ªçi train)

    # T√≥m t·∫Øt
    # ... (In k·∫øt qu·∫£)
```

- **M·ª•c ƒë√≠ch:** H√†m main ƒëi·ªÅu ph·ªëi to√†n b·ªô file `train_hgt.py`. K·∫øt n·ªëi c√°c b∆∞·ªõc t·ª´ load d·ªØ li·ªáu -> x·ª≠ l√Ω -> t·∫°o model -> training -> b√°o c√°o.
